"""
This type stub file was generated by pyright.
"""

from typing import Dict, List, Optional, Union
from sagemaker.job import _Job
from sagemaker.session import Session
from sagemaker.inputs import BatchDataCaptureConfig
from sagemaker.workflow.entities import PipelineVariable
from sagemaker.workflow.pipeline_context import runnable_by_pipeline

"""
This type stub file was generated by pyright.
"""
class Transformer:
    """A class for handling creating and interacting with Amazon SageMaker transform jobs."""
    JOB_CLASS_NAME = ...
    def __init__(self, model_name: Union[str, PipelineVariable], instance_count: Union[int, PipelineVariable], instance_type: Union[str, PipelineVariable], strategy: Optional[Union[str, PipelineVariable]] = ..., assemble_with: Optional[Union[str, PipelineVariable]] = ..., output_path: Optional[Union[str, PipelineVariable]] = ..., output_kms_key: Optional[Union[str, PipelineVariable]] = ..., accept: Optional[Union[str, PipelineVariable]] = ..., max_concurrent_transforms: Optional[Union[int, PipelineVariable]] = ..., max_payload: Optional[Union[int, PipelineVariable]] = ..., tags: Optional[List[Dict[str, Union[str, PipelineVariable]]]] = ..., env: Optional[Dict[str, Union[str, PipelineVariable]]] = ..., base_transform_job_name: Optional[str] = ..., sagemaker_session: Optional[Session] = ..., volume_kms_key: Optional[Union[str, PipelineVariable]] = ...) -> None:
        """Initialize a ``Transformer``.

        Args:
            model_name (str or PipelineVariable): Name of the SageMaker model being
                used for the transform job.
            instance_count (int or PipelineVariable): Number of EC2 instances to use.
            instance_type (str or PipelineVariable): Type of EC2 instance to use, for example,
                'ml.c4.xlarge'.
            strategy (str or PipelineVariable): The strategy used to decide how to batch records
                in a single request (default: None). Valid values: 'MultiRecord'
                and 'SingleRecord'.
            assemble_with (str or PipelineVariable): How the output is assembled (default: None).
                Valid values: 'Line' or 'None'.
            output_path (str or PipelineVariable): S3 location for saving the transform result. If
                not specified, results are stored to a default bucket.
            output_kms_key (str or PipelineVariable): Optional. KMS key ID for encrypting the
                transform output (default: None).
            accept (str or PipelineVariable): The accept header passed by the client to
                the inference endpoint. If it is supported by the endpoint,
                it will be the format of the batch transform output.
            max_concurrent_transforms (int or PipelineVariable): The maximum number of HTTP requests
                to be made to each individual transform container at one time.
            max_payload (int or PipelineVariable): Maximum size of the payload in a single HTTP
                request to the container in MB.
            tags (list[dict[str, str] or list[dict[str, PipelineVariable]]): List of tags for
                labeling a transform job (default: None). For more, see the SageMaker API
                documentation for `Tag <https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html>`_.
            env (dict[str, str] or dict[str, PipelineVariable]): Environment variables to be set
                for use during the transform job (default: None).
            base_transform_job_name (str): Prefix for the transform job when the
                :meth:`~sagemaker.transformer.Transformer.transform` method
                launches. If not specified, a default prefix will be generated
                based on the training image name that was used to train the
                model associated with the transform job.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, the estimator creates one
                using the default AWS configuration chain.
            volume_kms_key (str or PipelineVariable): Optional. KMS key ID for encrypting
                the volume attached to the ML compute instance (default: None).
        """
        ...
    
    @runnable_by_pipeline
    def transform(self, data: Union[str, PipelineVariable], data_type: Union[str, PipelineVariable] = ..., content_type: Optional[Union[str, PipelineVariable]] = ..., compression_type: Optional[Union[str, PipelineVariable]] = ..., split_type: Optional[Union[str, PipelineVariable]] = ..., job_name: Optional[str] = ..., input_filter: Optional[Union[str, PipelineVariable]] = ..., output_filter: Optional[Union[str, PipelineVariable]] = ..., join_source: Optional[Union[str, PipelineVariable]] = ..., experiment_config: Optional[Dict[str, str]] = ..., model_client_config: Optional[Dict[str, Union[str, PipelineVariable]]] = ..., batch_data_capture_config: BatchDataCaptureConfig = ..., wait: bool = ..., logs: bool = ...):
        """Start a new transform job.

        Args:
            data (str or PipelineVariable): Input data location in S3.
            data_type (str or PipelineVariable): What the S3 location defines (default: 'S3Prefix').
                Valid values:

                * 'S3Prefix' - the S3 URI defines a key name prefix. All objects with this prefix
                    will be used as inputs for the transform job.

                * 'ManifestFile' - the S3 URI points to a single manifest file listing each S3
                    object to use as an input for the transform job.

            content_type (str or PipelineVariable): MIME type of the input data (default: None).
            compression_type (str or PipelineVariable): Compression type of the input data, if
                compressed (default: None). Valid values: 'Gzip', None.
            split_type (str or PipelineVariable): The record delimiter for the input object
                (default: 'None'). Valid values: 'None', 'Line', 'RecordIO', and
                'TFRecord'.
            job_name (str): job name (default: None). If not specified, one will
                be generated.
            input_filter (str or PipelineVariable): A JSONPath to select a portion of the input to
                pass to the algorithm container for inference. If you omit the
                field, it gets the value '$', representing the entire input.
                For CSV data, each row is taken as a JSON array,
                so only index-based JSONPaths can be applied, e.g. $[0], $[1:].
                CSV data should follow the `RFC format <https://tools.ietf.org/html/rfc4180>`_.
                See `Supported JSONPath Operators
                <https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html#data-processing-operators>`_
                for a table of supported JSONPath operators.
                For more information, see the SageMaker API documentation for
                `CreateTransformJob
                <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html>`_.
                Some examples: "$[1:]", "$.features" (default: None).
            output_filter (str or PipelineVariable): A JSONPath to select a portion of the
                joined/original output to return as the output.
                For more information, see the SageMaker API documentation for
                `CreateTransformJob
                <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html>`_.
                Some examples: "$[1:]", "$.prediction" (default: None).
            join_source (str or PipelineVariable): The source of data to be joined to the transform
                output. It can be set to 'Input' meaning the entire input record
                will be joined to the inference result. You can use OutputFilter
                to select the useful portion before uploading to S3. (default:
                None). Valid values: Input, None.
            experiment_config (dict[str, str]): Experiment management configuration.
                Optionally, the dict can contain three keys:
                'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.
                The behavior of setting these keys is as follows:
                * If `ExperimentName` is supplied but `TrialName` is not a Trial will be
                automatically created and the job's Trial Component associated with the Trial.
                * If `TrialName` is supplied and the Trial already exists the job's Trial Component
                will be associated with the Trial.
                * If both `ExperimentName` and `TrialName` are not supplied the trial component
                will be unassociated.
                * `TrialComponentDisplayName` is used for display in Studio.
                * Both `ExperimentName` and `TrialName` will be ignored if the Transformer instance
                is built with :class:`~sagemaker.workflow.pipeline_context.PipelineSession`.
                However, the value of `TrialComponentDisplayName` is honored for display in Studio.
            model_client_config (dict[str, str] or dict[str, PipelineVariable]): Model
                configuration. Dictionary contains two optional keys,
                'InvocationsTimeoutInSeconds', and 'InvocationsMaxRetries'.
                (default: ``None``).
            batch_data_capture_config (BatchDataCaptureConfig): Configuration object which
                specifies the configurations related to the batch data capture for the transform job
                (default: ``None``).
            batch_data_capture_config (BatchDataCaptureConfig): Configuration object which
                specifies the configurations related to the batch data capture for the transform job
                (default: ``None``).
            wait (bool): Whether the call should wait until the job completes
                (default: ``True``).
            logs (bool): Whether to show the logs produced by the job.
                Only meaningful when wait is ``True`` (default: ``True``).
        Returns:
            None or pipeline step arguments in case the Transformer instance is built with
            :class:`~sagemaker.workflow.pipeline_context.PipelineSession`
        """
        ...
    
    def transform_with_monitoring(self, monitoring_config, monitoring_resource_config, data: str, data_type: str = ..., content_type: str = ..., compression_type: str = ..., split_type: str = ..., input_filter: str = ..., output_filter: str = ..., join_source: str = ..., model_client_config: Dict[str, str] = ..., batch_data_capture_config: BatchDataCaptureConfig = ..., monitor_before_transform: bool = ..., supplied_baseline_statistics: str = ..., supplied_baseline_constraints: str = ..., wait: bool = ..., pipeline_name: str = ..., role: str = ...):
        """Runs a transform job with monitoring job.

        Note that this function will not start a transform job immediately,
        instead, it will create a SageMaker Pipeline and execute it.
        If you provide an existing pipeline_name, no new pipeline will be created, otherwise,
        each transform_with_monitoring call will create a new pipeline and execute.

        Args:
            monitoring_config (Union[
                `sagemaker.workflow.quality_check_step.QualityCheckConfig`,
                `sagemaker.workflow.quality_check_step.ClarifyCheckConfig`
            ]): the monitoring configuration used for run model monitoring.
            monitoring_resource_config (`sagemaker.workflow.check_job_config.CheckJobConfig`):
                the check job (processing job) cluster resource configuration.
            transform_step_args (_JobStepArguments): the transform step transform arguments.
            data (str): Input data location in S3 for the transform job
            data_type (str): What the S3 location defines (default: 'S3Prefix').
                Valid values:
                * 'S3Prefix' - the S3 URI defines a key name prefix. All objects with this prefix
                will be used as inputs for the transform job.
                * 'ManifestFile' - the S3 URI points to a single manifest file listing each S3
                object to use as an input for the transform job.
            content_type (str): MIME type of the input data (default: None).
            compression_type (str): Compression type of the input data, if
                compressed (default: None). Valid values: 'Gzip', None.
            split_type (str): The record delimiter for the input object
                (default: 'None'). Valid values: 'None', 'Line', 'RecordIO', and
                'TFRecord'.
            input_filter (str): A JSONPath to select a portion of the input to
                pass to the algorithm container for inference. If you omit the
                field, it gets the value '$', representing the entire input.
                For CSV data, each row is taken as a JSON array,
                so only index-based JSONPaths can be applied, e.g. $[0], $[1:].
                CSV data should follow the `RFC format <https://tools.ietf.org/html/rfc4180>`_.
                See `Supported JSONPath Operators
                <https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html#data-processing-operators>`_
                for a table of supported JSONPath operators.
                For more information, see the SageMaker API documentation for
                `CreateTransformJob
                <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html>`_.
                Some examples: "$[1:]", "$.features" (default: None).
            output_filter (str): A JSONPath to select a portion of the
                joined/original output to return as the output.
                For more information, see the SageMaker API documentation for
                `CreateTransformJob
                <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTransformJob.html>`_.
                Some examples: "$[1:]", "$.prediction" (default: None).
            join_source (str): The source of data to be joined to the transform
                output. It can be set to 'Input' meaning the entire input record
                will be joined to the inference result. You can use OutputFilter
                to select the useful portion before uploading to S3. (default:
                None). Valid values: Input, None.
            model_client_config (dict[str, str]): Model configuration.
                Dictionary contains two optional keys,
                'InvocationsTimeoutInSeconds', and 'InvocationsMaxRetries'.
                (default: ``None``).
            batch_data_capture_config (BatchDataCaptureConfig): Configuration object which
                specifies the configurations related to the batch data capture for the transform job
                (default: ``None``).
            monitor_before_transform (bgool): If to run data quality
                or model explainability monitoring type,
                a true value of this flag indicates running the check step before the transform job.
            fail_on_violation (Union[bool, PipelineVariable]): A opt-out flag to not to fail the
                check step when a violation is detected.
            supplied_baseline_statistics (Union[str, PipelineVariable]): The S3 path
                to the supplied statistics object representing the statistics JSON file
                which will be used for drift to check (default: None).
            supplied_baseline_constraints (Union[str, PipelineVariable]): The S3 path
                to the supplied constraints object representing the constraints JSON file
                which will be used for drift to check (default: None).
            wait (bool): To determine if needed to wait for the pipeline execution to complete
            pipeline_name (str): The name of the Pipeline for the monitoring and transfrom step
            role (str): Execution role
        """
        ...
    
    def delete_model(self):
        """Delete the corresponding SageMaker model for this Transformer."""
        ...
    
    def wait(self, logs=...):
        """Placeholder docstring"""
        ...
    
    def stop_transform_job(self, wait=...):
        """Stop latest running batch transform job."""
        ...
    
    @classmethod
    def attach(cls, transform_job_name, sagemaker_session=...):
        """Attach an existing transform job to a new Transformer instance

        Args:
            transform_job_name (str): Name for the transform job to be attached.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one will be created using
                the default AWS configuration chain.

        Returns:
            sagemaker.transformer.Transformer: The Transformer instance with the
            specified transform job attached.
        """
        ...
    


class _TransformJob(_Job):
    """Placeholder docstring"""
    @classmethod
    def start_new(cls, transformer, data, data_type, content_type, compression_type, split_type, input_filter, output_filter, join_source, experiment_config, model_client_config, batch_data_capture_config):
        """Placeholder docstring"""
        ...
    
    def wait(self, logs=...):
        ...
    
    def stop(self):
        """Placeholder docstring"""
        ...
    


