"""
This type stub file was generated by pyright.
"""

import attr
from typing import Optional, Union
from sagemaker.model_monitor.dataset_format import MonitoringDatasetFormat
from sagemaker.processing import ProcessingJob

"""
This type stub file was generated by pyright.
"""
DEFAULT_REPOSITORY_NAME = ...
STATISTICS_JSON_DEFAULT_FILE_NAME = ...
CONSTRAINTS_JSON_DEFAULT_FILE_NAME = ...
CONSTRAINT_VIOLATIONS_JSON_DEFAULT_FILE_NAME = ...
_CONTAINER_BASE_PATH = ...
_CONTAINER_INPUT_PATH = ...
_CONTAINER_ENDPOINT_INPUT_PATH = ...
_BASELINE_DATASET_INPUT_NAME = ...
_RECORD_PREPROCESSOR_SCRIPT_INPUT_NAME = ...
_POST_ANALYTICS_PROCESSOR_SCRIPT_INPUT_NAME = ...
_CONTAINER_OUTPUT_PATH = ...
_DEFAULT_OUTPUT_NAME = ...
_MODEL_MONITOR_S3_PATH = ...
_BASELINING_S3_PATH = ...
_MONITORING_S3_PATH = ...
_RESULTS_S3_PATH = ...
_INPUT_S3_PATH = ...
_SUGGESTION_JOB_BASE_NAME = ...
_MONITORING_SCHEDULE_BASE_NAME = ...
_DATASET_SOURCE_PATH_ENV_NAME = ...
_DATASET_FORMAT_ENV_NAME = ...
_OUTPUT_PATH_ENV_NAME = ...
_RECORD_PREPROCESSOR_SCRIPT_ENV_NAME = ...
_POST_ANALYTICS_PROCESSOR_SCRIPT_ENV_NAME = ...
_PUBLISH_CLOUDWATCH_METRICS_ENV_NAME = ...
_ANALYSIS_TYPE_ENV_NAME = ...
_PROBLEM_TYPE_ENV_NAME = ...
_GROUND_TRUTH_ATTRIBUTE_ENV_NAME = ...
_INFERENCE_ATTRIBUTE_ENV_NAME = ...
_PROBABILITY_ATTRIBUTE_ENV_NAME = ...
_PROBABILITY_THRESHOLD_ATTRIBUTE_ENV_NAME = ...
_CATEGORICAL_DRIFT_METHOD_ENV_NAME = ...
_LOGGER = ...
framework_name = ...
class ModelMonitor:
    """Sets up Amazon SageMaker Monitoring Schedules and baseline suggestions.

    Use this class when you want to provide your own container image containing the code
    you'd like to run, in order to produce your own statistics and constraint validation files.
    For a more guided experience, consider using the DefaultModelMonitor class instead.
    """
    def __init__(self, role=..., image_uri=..., instance_count=..., instance_type=..., entrypoint=..., volume_size_in_gb=..., volume_kms_key=..., output_kms_key=..., max_runtime_in_seconds=..., base_job_name=..., sagemaker_session=..., env=..., tags=..., network_config=...) -> None:
        """Initializes a ``Monitor`` instance.

        The Monitor handles baselining datasets and creating Amazon SageMaker Monitoring Schedules
        to monitor SageMaker endpoints.

        Args:
            role (str): An AWS IAM role. The Amazon SageMaker jobs use this role.
            image_uri (str): The uri of the image to use for the jobs started by
                the Monitor.
            instance_count (int): The number of instances to run
                the jobs with.
            instance_type (str): Type of EC2 instance to use for
                the job, for example, 'ml.m5.xlarge'.
            entrypoint ([str]): The entrypoint for the job.
            volume_size_in_gb (int): Size in GB of the EBS volume
                to use for storing data during processing (default: 30).
            volume_kms_key (str): A KMS key for the job's volume.
            output_kms_key (str): The KMS key id for the job's outputs.
            max_runtime_in_seconds (int): Timeout in seconds. After this amount of
                time, Amazon SageMaker terminates the job regardless of its current status.
                Default: 3600
            base_job_name (str): Prefix for the job name. If not specified,
                a default name is generated based on the training image name and
                current timestamp.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one is created using
                the default AWS configuration chain.
            env (dict): Environment variables to be passed to the job.
            tags ([dict]): List of tags to be passed to the job.
            network_config (sagemaker.network.NetworkConfig): A NetworkConfig
                object that configures network isolation, encryption of
                inter-container traffic, security group IDs, and subnets.

        """
        ...
    
    def run_baseline(self, baseline_inputs, output, arguments=..., wait=..., logs=..., job_name=...):
        """Run a processing job meant to baseline your dataset.

        Args:
            baseline_inputs ([sagemaker.processing.ProcessingInput]): Input files for the processing
                job. These must be provided as ProcessingInput objects.
            output (sagemaker.processing.ProcessingOutput): Destination of the constraint_violations
                and statistics json files.
            arguments ([str]): A list of string arguments to be passed to a processing job.
            wait (bool): Whether the call should wait until the job completes (default: True).
            logs (bool): Whether to show the logs produced by the job.
                Only meaningful when wait is True (default: True).
            job_name (str): Processing job name. If not specified, the processor generates
                a default job name, based on the image name and current timestamp.

        """
        ...
    
    def create_monitoring_schedule(self, endpoint_input=..., output=..., statistics=..., constraints=..., monitor_schedule_name=..., schedule_cron_expression=..., batch_transform_input=..., arguments=..., data_analysis_start_time=..., data_analysis_end_time=...):
        """Creates a monitoring schedule to monitor an Amazon SageMaker Endpoint.

        If constraints and statistics are provided, or if they are able to be retrieved from a
        previous baselining job associated with this monitor, those will be used.
        If constraints and statistics cannot be automatically retrieved, baseline_inputs will be
        required in order to kick off a baselining job.

        Args:
            endpoint_input (str or sagemaker.model_monitor.EndpointInput): The endpoint to monitor.
                This can either be the endpoint name or an EndpointInput. (default: None)
            output (sagemaker.model_monitor.MonitoringOutput): The output of the monitoring
                schedule. (default: None)
            statistics (sagemaker.model_monitor.Statistic or str): If provided alongside
                constraints, these will be used for monitoring the endpoint. This can be a
                sagemaker.model_monitor.Statistic object or an S3 uri pointing to a statistic
                JSON file. (default: None)
            constraints (sagemaker.model_monitor.Constraints or str): If provided alongside
                statistics, these will be used for monitoring the endpoint. This can be a
                sagemaker.model_monitor.Constraints object or an S3 uri pointing to a constraints
                JSON file. (default: None)
            monitor_schedule_name (str): Schedule name. If not specified, the processor generates
                a default job name, based on the image name and current timestamp. (default: None)
            schedule_cron_expression (str): The cron expression that dictates the frequency that
                this job runs at. See sagemaker.model_monitor.CronExpressionGenerator for valid
                expressions. Default: Daily. (default: None)
            batch_transform_input (sagemaker.model_monitor.BatchTransformInput): Inputs to
                run the monitoring schedule on the batch transform
                (default: None)
            arguments ([str]): A list of string arguments to be passed to a processing job.
            data_analysis_start_time (str): Start time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)
            data_analysis_end_time (str): End time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)

        """
        ...
    
    def update_monitoring_schedule(self, endpoint_input=..., output=..., statistics=..., constraints=..., schedule_cron_expression=..., instance_count=..., instance_type=..., entrypoint=..., volume_size_in_gb=..., volume_kms_key=..., output_kms_key=..., arguments=..., max_runtime_in_seconds=..., env=..., network_config=..., role=..., image_uri=..., batch_transform_input=..., data_analysis_start_time=..., data_analysis_end_time=...):
        """Updates the existing monitoring schedule.

        If more options than schedule_cron_expression are to be updated, a new job definition will
        be created to hold them. The old job definition will not be deleted.

        Args:
            endpoint_input (str or sagemaker.model_monitor.EndpointInput): The endpoint to monitor.
                This can either be the endpoint name or an EndpointInput.
            output (sagemaker.model_monitor.MonitoringOutput): The output of the monitoring
                schedule.
            statistics (sagemaker.model_monitor.Statistic or str): If provided alongside
                constraints, these will be used for monitoring the endpoint. This can be a
                sagemaker.model_monitor.Statistics object or an S3 uri pointing to a statistics
                JSON file.
            constraints (sagemaker.model_monitor.Constraints or str): If provided alongside
                statistics, these will be used for monitoring the endpoint. This can be a
                sagemaker.model_monitor.Constraints object or an S3 uri pointing to a constraints
                JSON file.
            schedule_cron_expression (str): The cron expression that dictates the frequency that
                this job runs at.  See sagemaker.model_monitor.CronExpressionGenerator for valid
                expressions.
            instance_count (int): The number of instances to run
                the jobs with.
            instance_type (str): Type of EC2 instance to use for
                the job, for example, 'ml.m5.xlarge'.
            entrypoint (str): The entrypoint for the job.
            volume_size_in_gb (int): Size in GB of the EBS volume
                to use for storing data during processing (default: 30).
            volume_kms_key (str): A KMS key for the job's volume.
            output_kms_key (str): The KMS key id for the job's outputs.
            arguments ([str]): A list of string arguments to be passed to a processing job.
            max_runtime_in_seconds (int): Timeout in seconds. After this amount of
                time, Amazon SageMaker terminates the job regardless of its current status.
                Default: 3600
            env (dict): Environment variables to be passed to the job.
            network_config (sagemaker.network.NetworkConfig): A NetworkConfig
                object that configures network isolation, encryption of
                inter-container traffic, security group IDs, and subnets.
            role (str): An AWS IAM role name or ARN. The Amazon SageMaker jobs use this role.
            image_uri (str): The uri of the image to use for the jobs started by
                the Monitor.
            batch_transform_input (sagemaker.model_monitor.BatchTransformInput): Inputs to
                run the monitoring schedule on the batch transform (default: None)
            data_analysis_start_time (str): Start time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)
            data_analysis_end_time (str): End time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)

        """
        ...
    
    def start_monitoring_schedule(self):
        """Starts the monitoring schedule."""
        ...
    
    def stop_monitoring_schedule(self):
        """Stops the monitoring schedule."""
        ...
    
    def delete_monitoring_schedule(self):
        """Deletes the monitoring schedule (subclass is responsible for deleting job definition)"""
        ...
    
    def baseline_statistics(self, file_name=...):
        """Returns a Statistics object representing the statistics json file

        Object is generated by the latest baselining job.

        Args:
            file_name (str): The name of the .json statistics file

        Returns:
            sagemaker.model_monitor.Statistics: The Statistics object representing the file that
                was generated by the job.

        """
        ...
    
    def suggested_constraints(self, file_name=...):
        """Returns a Statistics object representing the constraints json file.

        Object is generated by the latest baselining job

        Args:
            file_name (str): The name of the .json constraints file

        Returns:
            sagemaker.model_monitor.Constraints: The Constraints object representing the file that
                was generated by the job.

        """
        ...
    
    def latest_monitoring_statistics(self, file_name=...):
        """Returns the sagemaker.model_monitor.

        Statistics generated by the latest monitoring execution.

        Args:
            file_name (str): The name of the statistics file to be retrieved. Only override if
                generating a custom file name.

        Returns:
            sagemaker.model_monitoring.Statistics: The Statistics object representing the file
                generated by the latest monitoring execution.

        """
        ...
    
    def latest_monitoring_constraint_violations(self, file_name=...):
        """Returns the sagemaker.model_monitor.

        ConstraintViolations generated by the latest monitoring execution.

        Args:
            file_name (str): The name of the constraint violdations file to be retrieved. Only
                override if generating a custom file name.

        Returns:
            sagemaker.model_monitoring.ConstraintViolations: The ConstraintViolations object
                representing the file generated by the latest monitoring execution.

        """
        ...
    
    def describe_latest_baselining_job(self):
        """Describe the latest baselining job kicked off by the suggest workflow."""
        ...
    
    def describe_schedule(self):
        """Describes the schedule that this object represents.

        Returns:
            dict: A dictionary response with the monitoring schedule description.

        """
        ...
    
    def list_executions(self):
        """Get the list of the latest monitoring executions in descending order of "ScheduledTime".

        Statistics or violations can be called following this example:
        Example:
            >>> my_executions = my_monitor.list_executions()
            >>> second_to_last_execution_statistics = my_executions[-1].statistics()
            >>> second_to_last_execution_violations = my_executions[-1].constraint_violations()

        Returns:
            [sagemaker.model_monitor.MonitoringExecution]: List of MonitoringExecutions in
                descending order of "ScheduledTime".

        """
        ...
    
    def get_latest_execution_logs(self, wait=...):
        """Get the processing job logs for the most recent monitoring execution

        Args:
            wait (bool): Whether the call should wait until the job completes (default: False).

        Raises:
            ValueError: If no execution job or processing job for the last execution has run

        Returns: None
        """
        ...
    
    def update_monitoring_alert(self, monitoring_alert_name: str, data_points_to_alert: Optional[int], evaluation_period: Optional[int]):
        """Update the monitoring schedule alert.

         Args:
            monitoring_alert_name (str): The name of the monitoring alert to update.
            data_points_to_alert (int):  The data point to alert.
            evaluation_period (int): The period to evaluate the alert status.

        Returns: None
        """
        ...
    
    def list_monitoring_alerts(self, next_token: Optional[str] = ..., max_results: Optional[int] = ...):
        """List the monitoring alerts.

        Args:
             next_token (Optional[str]):  The pagination token. Default: None
             max_results (Optional[int]): The maximum number of results to return.
             Must be between 1 and 100. Default: 10

        Returns:
             List[MonitoringAlertSummary]: list of monitoring alert history.
             str: Next token.
        """
        ...
    
    def list_monitoring_alert_history(self, monitoring_alert_name: Optional[str] = ..., sort_by: Optional[str] = ..., sort_order: Optional[str] = ..., next_token: Optional[str] = ..., max_results: Optional[int] = ..., creation_time_before: Optional[str] = ..., creation_time_after: Optional[str] = ..., status_equals: Optional[str] = ...):
        """Lists the alert history associated with the given schedule_name and alert_name.

        Args:
            monitoring_alert_name (Optional[str]): The name of the alert_name to filter on.
                If not provided, does not filter on it. Default: None.
            sort_by (Optional[str]): sort_by (str): The field to sort by.
                Can be one of: "Name", "CreationTime"
                Default: "CreationTime".
            sort_order (Optional[str]): The sort order. Can be one of: "Ascending", "Descending".
                Default: "Descending".
            next_token (Optional[str]):  The pagination token. Default: None.
            max_results (Optional[int]): The maximum number of results to return.
                Must be between 1 and 100. Default: 10.
            creation_time_before (Optional[str]): A filter to filter alert history before a time
                Default: None.
            creation_time_after (Optional[str]): A filter to filter alert history after a time
                Default: None.
            status_equals (Optional[str]): A filter to filter alert history by status
                Default: None.
        Returns:
            List[MonitoringAlertHistorySummary]: list of monitoring alert history.
            str: Next token.
        """
        ...
    
    @classmethod
    def attach(cls, monitor_schedule_name, sagemaker_session=...):
        """Set this object's schedule name point to the Amazon Sagemaker Monitoring Schedule name.

        This allows subsequent describe_schedule or list_executions calls to point
        to the given schedule.

        Args:
            monitor_schedule_name (str): The name of the schedule to attach to.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one is created using
                the default AWS configuration chain.

        """
        ...
    
    @classmethod
    def monitoring_type(cls):
        """Type of the monitoring job."""
        ...
    


class DefaultModelMonitor(ModelMonitor):
    """Sets up Amazon SageMaker Monitoring Schedules and baseline suggestions.

    Use this class when you want to utilize Amazon SageMaker Monitoring's plug-and-play
    solution that only requires your dataset and optional pre/postprocessing scripts.
    For a more customized experience, consider using the ModelMonitor class instead.
    """
    JOB_DEFINITION_BASE_NAME = ...
    def __init__(self, role=..., instance_count=..., instance_type=..., volume_size_in_gb=..., volume_kms_key=..., output_kms_key=..., max_runtime_in_seconds=..., base_job_name=..., sagemaker_session=..., env=..., tags=..., network_config=...) -> None:
        """Initializes a ``Monitor`` instance.

        The Monitor handles baselining datasets and creating Amazon SageMaker Monitoring
        Schedules to monitor SageMaker endpoints.

        Args:
            role (str): An AWS IAM role name or ARN. The Amazon SageMaker jobs use this role.
            instance_count (int): The number of instances to run the jobs with.
            instance_type (str): Type of EC2 instance to use for the job, for example,
                'ml.m5.xlarge'.
            volume_size_in_gb (int): Size in GB of the EBS volume
                to use for storing data during processing (default: 30).
            volume_kms_key (str): A KMS key for the processing volume.
            output_kms_key (str): The KMS key id for the job's outputs.
            max_runtime_in_seconds (int): Timeout in seconds. After this amount of
                time, Amazon SageMaker terminates the job regardless of its current status.
                Default: 3600
            base_job_name (str): Prefix for the job name. If not specified,
                a default name is generated based on the training image name and
                current timestamp.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one is created using
                the default AWS configuration chain.
            env (dict): Environment variables to be passed to the job.
            tags ([dict]): List of tags to be passed to the job.
            network_config (sagemaker.network.NetworkConfig): A NetworkConfig
                object that configures network isolation, encryption of
                inter-container traffic, security group IDs, and subnets.

        """
        ...
    
    @classmethod
    def monitoring_type(cls):
        """Type of the monitoring job."""
        ...
    
    def suggest_baseline(self, baseline_dataset, dataset_format, record_preprocessor_script=..., post_analytics_processor_script=..., output_s3_uri=..., wait=..., logs=..., job_name=..., monitoring_config_override=...):
        """Suggest baselines for use with Amazon SageMaker Model Monitoring Schedules.

        Args:
            baseline_dataset (str): The path to the baseline_dataset file. This can be a local path
                or an S3 uri.
            dataset_format (dict): The format of the baseline_dataset.
            record_preprocessor_script (str): The path to the record preprocessor script. This can
                be a local path or an S3 uri.
            post_analytics_processor_script (str): The path to the record post-analytics processor
                script. This can be a local path or an S3 uri.
            output_s3_uri (str): Desired S3 destination Destination of the constraint_violations
                and statistics json files.
                Default: "s3://<default_session_bucket>/<job_name>/output"
            wait (bool): Whether the call should wait until the job completes (default: True).
            logs (bool): Whether to show the logs produced by the job.
                Only meaningful when wait is True (default: True).
            job_name (str): Processing job name. If not specified, the processor generates
                a default job name, based on the image name and current timestamp.
            monitoring_config_override (DataQualityMonitoringConfig): monitoring_config object to
                override the global monitoring_config parameter of constraints suggested by
                Model Monitor Container. If not specified, the values suggested by container is
                set.
        Returns:
            sagemaker.processing.ProcessingJob: The ProcessingJob object representing the
                baselining job.

        """
        ...
    
    def create_monitoring_schedule(self, endpoint_input=..., record_preprocessor_script=..., post_analytics_processor_script=..., output_s3_uri=..., constraints=..., statistics=..., monitor_schedule_name=..., schedule_cron_expression=..., enable_cloudwatch_metrics=..., batch_transform_input=..., data_analysis_start_time=..., data_analysis_end_time=...):
        """Creates a monitoring schedule to monitor an Amazon SageMaker Endpoint.

        If constraints and statistics are provided, or if they are able to be retrieved from a
        previous baselining job associated with this monitor, those will be used.
        If constraints and statistics cannot be automatically retrieved, baseline_inputs will be
        required in order to kick off a baselining job.

        Args:
            endpoint_input (str or sagemaker.model_monitor.EndpointInput): The endpoint to monitor.
                This can either be the endpoint name or an EndpointInput. (default: None)
            record_preprocessor_script (str): The path to the record preprocessor script. This can
                be a local path or an S3 uri.
            post_analytics_processor_script (str): The path to the record post-analytics processor
                script. This can be a local path or an S3 uri.
            output_s3_uri (str): Desired S3 destination of the constraint_violations and
                statistics json files.
                Default: "s3://<default_session_bucket>/<job_name>/output"
            constraints (sagemaker.model_monitor.Constraints or str): If provided alongside
                statistics, these will be used for monitoring the endpoint. This can be a
                sagemaker.model_monitor.Constraints object or an s3_uri pointing to a constraints
                JSON file.
            statistics (sagemaker.model_monitor.Statistic or str): If provided alongside
                constraints, these will be used for monitoring the endpoint. This can be a
                sagemaker.model_monitor.Statistics object or an s3_uri pointing to a statistics
                JSON file.
            monitor_schedule_name (str): Schedule name. If not specified, the processor generates
                a default job name, based on the image name and current timestamp.
            schedule_cron_expression (str): The cron expression that dictates the frequency that
                this job run. See sagemaker.model_monitor.CronExpressionGenerator for valid
                expressions. Default: Daily.
            enable_cloudwatch_metrics (bool): Whether to publish cloudwatch metrics as part of
                the baselining or monitoring jobs.
            batch_transform_input (sagemaker.model_monitor.BatchTransformInput): Inputs to
                run the monitoring schedule on the batch transform (default: None)
            data_analysis_start_time (str): Start time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)
            data_analysis_end_time (str): End time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)
        """
        ...
    
    def update_monitoring_schedule(self, endpoint_input=..., record_preprocessor_script=..., post_analytics_processor_script=..., output_s3_uri=..., statistics=..., constraints=..., schedule_cron_expression=..., instance_count=..., instance_type=..., volume_size_in_gb=..., volume_kms_key=..., output_kms_key=..., max_runtime_in_seconds=..., env=..., network_config=..., enable_cloudwatch_metrics=..., role=..., batch_transform_input=..., data_analysis_start_time=..., data_analysis_end_time=...):
        """Updates the existing monitoring schedule.

        Args:
            endpoint_input (str or sagemaker.model_monitor.EndpointInput): The endpoint to monitor.
                This can either be the endpoint name or an EndpointInput.
            record_preprocessor_script (str): The path to the record preprocessor script. This can
                be a local path or an S3 uri.
            post_analytics_processor_script (str): The path to the record post-analytics processor
                script. This can be a local path or an S3 uri.
            output_s3_uri (str): Desired S3 destination of the constraint_violations and
                statistics json files.
            statistics (sagemaker.model_monitor.Statistic or str): If provided alongside
                constraints, these will be used for monitoring the endpoint. This can be a
                sagemaker.model_monitor.Statistics object or an S3 uri pointing to a statistics
                JSON file.
            constraints (sagemaker.model_monitor.Constraints or str): If provided alongside
                statistics, these will be used for monitoring the endpoint. This can be a
                sagemaker.model_monitor.Constraints object or an S3 uri pointing to a constraints
                JSON file.
            schedule_cron_expression (str): The cron expression that dictates the frequency that
                this job runs at. See sagemaker.model_monitor.CronExpressionGenerator for valid
                expressions.
            instance_count (int): The number of instances to run
                the jobs with.
            instance_type (str): Type of EC2 instance to use for
                the job, for example, 'ml.m5.xlarge'.
            volume_size_in_gb (int): Size in GB of the EBS volume
                to use for storing data during processing (default: 30).
            volume_kms_key (str): A KMS key for the job's volume.
            output_kms_key (str): The KMS key id for the job's outputs.
            max_runtime_in_seconds (int): Timeout in seconds. After this amount of
                time, Amazon SageMaker terminates the job regardless of its current status.
                Default: 3600
            env (dict): Environment variables to be passed to the job.
            network_config (sagemaker.network.NetworkConfig): A NetworkConfig
                object that configures network isolation, encryption of
                inter-container traffic, security group IDs, and subnets.
            enable_cloudwatch_metrics (bool): Whether to publish cloudwatch metrics as part of
                the baselining or monitoring jobs.
            role (str): An AWS IAM role name or ARN. The Amazon SageMaker jobs use this role.
            batch_transform_input (sagemaker.model_monitor.BatchTransformInput): Inputs to
                run the monitoring schedule on the batch transform (default: None)
            data_analysis_start_time (str): Start time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)
            data_analysis_end_time (str): End time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)

        """
        ...
    
    def delete_monitoring_schedule(self):
        """Deletes the monitoring schedule and its job definition."""
        ...
    
    def run_baseline(self):
        """Not implemented.

        '.run_baseline()' is only allowed for ModelMonitor objects. Please use
        `suggest_baseline` for DefaultModelMonitor objects, instead.

        Raises:
            NotImplementedError
        """
        ...
    
    @classmethod
    def attach(cls, monitor_schedule_name, sagemaker_session=...):
        """Sets this object's schedule name to the name provided.

        This allows subsequent describe_schedule or list_executions calls to point
        to the given schedule.

        Args:
            monitor_schedule_name (str): The name of the schedule to attach to.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one is created using
                the default AWS configuration chain.
        """
        ...
    
    def latest_monitoring_statistics(self):
        """Returns the sagemaker.model_monitor.Statistics.

        These are the statistics generated by the latest monitoring execution.

        Returns:
            sagemaker.model_monitoring.Statistics: The Statistics object representing the file
                generated by the latest monitoring execution.

        """
        ...
    
    def latest_monitoring_constraint_violations(self):
        """Returns the sagemaker.model_monitor.

        ConstraintViolations generated by the latest monitoring execution.

        Returns:
            sagemaker.model_monitoring.ConstraintViolations: The ConstraintViolations object
                representing the file generated by the latest monitoring execution.

        """
        ...
    


class ModelQualityMonitor(ModelMonitor):
    """Amazon SageMaker model monitor to monitor quality metrics for an endpoint.

    Please see the __init__ method of its base class for how to instantiate it.
    """
    JOB_DEFINITION_BASE_NAME = ...
    def __init__(self, role=..., instance_count=..., instance_type=..., volume_size_in_gb=..., volume_kms_key=..., output_kms_key=..., max_runtime_in_seconds=..., base_job_name=..., sagemaker_session=..., env=..., tags=..., network_config=...) -> None:
        """Initializes a monitor instance.

        The monitor handles baselining datasets and creating Amazon SageMaker
        Monitoring Schedules to monitor SageMaker endpoints.

        Args:
            role (str): An AWS IAM role. The Amazon SageMaker jobs use this role.
            instance_count (int): The number of instances to run
                the jobs with.
            instance_type (str): Type of EC2 instance to use for
                the job, for example, 'ml.m5.xlarge'.
            volume_size_in_gb (int): Size in GB of the EBS volume
                to use for storing data during processing (default: 30).
            volume_kms_key (str): A KMS key for the job's volume.
            output_kms_key (str): The KMS key id for the job's outputs.
            max_runtime_in_seconds (int): Timeout in seconds. After this amount of
                time, Amazon SageMaker terminates the job regardless of its current status.
                Default: 3600
            base_job_name (str): Prefix for the job name. If not specified,
                a default name is generated based on the training image name and
                current timestamp.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one is created using
                the default AWS configuration chain.
            env (dict): Environment variables to be passed to the job.
            tags ([dict]): List of tags to be passed to the job.
            network_config (sagemaker.network.NetworkConfig): A NetworkConfig
                object that configures network isolation, encryption of
                inter-container traffic, security group IDs, and subnets.
        """
        ...
    
    @classmethod
    def monitoring_type(cls):
        """Type of the monitoring job."""
        ...
    
    def suggest_baseline(self, baseline_dataset, dataset_format, problem_type, inference_attribute=..., probability_attribute=..., ground_truth_attribute=..., probability_threshold_attribute=..., post_analytics_processor_script=..., output_s3_uri=..., wait=..., logs=..., job_name=...):
        """Suggest baselines for use with Amazon SageMaker Model Monitoring Schedules.

        Args:
            baseline_dataset (str): The path to the baseline_dataset file. This can be a local
                path or an S3 uri.
            dataset_format (dict): The format of the baseline_dataset.
            problem_type (str): The type of problem of this model quality monitoring. Valid
                values are "Regression", "BinaryClassification", "MulticlassClassification".
            inference_attribute (str): Index or JSONpath to locate predicted label(s).
                Only used for ModelQualityMonitor.
            probability_attribute (str or int): Index or JSONpath to locate probabilities.
                Only used for ModelQualityMonitor.
            ground_truth_attribute (str): Index to locate actual label(s).
                Only used for ModelQualityMonitor.
            probability_threshold_attribute (float): threshold to convert probabilities to binaries
                Only used for ModelQualityMonitor.
            post_analytics_processor_script (str): The path to the record post-analytics processor
                script. This can be a local path or an S3 uri.
            output_s3_uri (str): Desired S3 destination Destination of the constraint_violations
                and statistics json files.
                Default: "s3://<default_session_bucket>/<job_name>/output"
            wait (bool): Whether the call should wait until the job completes (default: False).
            logs (bool): Whether to show the logs produced by the job.
                Only meaningful when wait is True (default: False).
            job_name (str): Processing job name. If not specified, the processor generates
                a default job name, based on the image name and current timestamp.

        Returns:
            sagemaker.processing.ProcessingJob: The ProcessingJob object representing the
                baselining job.

        """
        ...
    
    def create_monitoring_schedule(self, endpoint_input=..., ground_truth_input=..., problem_type=..., record_preprocessor_script=..., post_analytics_processor_script=..., output_s3_uri=..., constraints=..., monitor_schedule_name=..., schedule_cron_expression=..., enable_cloudwatch_metrics=..., batch_transform_input=..., data_analysis_start_time=..., data_analysis_end_time=...):
        """Creates a monitoring schedule.

        Args:
            endpoint_input (str or sagemaker.model_monitor.EndpointInput): The endpoint to
                monitor. This can either be the endpoint name or an EndpointInput.
                (default: None)
            ground_truth_input (str): S3 URI to ground truth dataset.
                (default: None)
            problem_type (str): The type of problem of this model quality monitoring. Valid
                values are "Regression", "BinaryClassification", "MulticlassClassification".
                (default: None)
            record_preprocessor_script (str): The path to the record preprocessor script. This can
                be a local path or an S3 uri.
            post_analytics_processor_script (str): The path to the record post-analytics processor
                script. This can be a local path or an S3 uri.
            output_s3_uri (str): S3 destination of the constraint_violations and analysis result.
                Default: "s3://<default_session_bucket>/<job_name>/output"
            constraints (sagemaker.model_monitor.Constraints or str): If provided it will be used
                for monitoring the endpoint. It can be a Constraints object or an S3 uri pointing
                to a constraints JSON file.
            monitor_schedule_name (str): Schedule name. If not specified, the processor generates
                a default job name, based on the image name and current timestamp.
            schedule_cron_expression (str): The cron expression that dictates the frequency that
                this job run. See sagemaker.model_monitor.CronExpressionGenerator for valid
                expressions. Default: Daily.
            enable_cloudwatch_metrics (bool): Whether to publish cloudwatch metrics as part of
                the baselining or monitoring jobs.
            batch_transform_input (sagemaker.model_monitor.BatchTransformInput): Inputs to
                run the monitoring schedule on the batch transform
            data_analysis_start_time (str): Start time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)
            data_analysis_end_time (str): End time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)
        """
        ...
    
    def update_monitoring_schedule(self, endpoint_input=..., ground_truth_input=..., problem_type=..., record_preprocessor_script=..., post_analytics_processor_script=..., output_s3_uri=..., constraints=..., schedule_cron_expression=..., enable_cloudwatch_metrics=..., role=..., instance_count=..., instance_type=..., volume_size_in_gb=..., volume_kms_key=..., output_kms_key=..., max_runtime_in_seconds=..., env=..., network_config=..., batch_transform_input=..., data_analysis_start_time=..., data_analysis_end_time=...):
        """Updates the existing monitoring schedule.

        If more options than schedule_cron_expression are to be updated, a new job definition will
        be created to hold them. The old job definition will not be deleted.

        Args:
            endpoint_input (str or sagemaker.model_monitor.EndpointInput): The endpoint
                to monitor. This can either be the endpoint name or an EndpointInput.
            ground_truth_input (str): S3 URI to ground truth dataset.
            problem_type (str): The type of problem of this model quality monitoring. Valid values
                are "Regression", "BinaryClassification", "MulticlassClassification".
            record_preprocessor_script (str): The path to the record preprocessor script. This can
                be a local path or an S3 uri.
            post_analytics_processor_script (str): The path to the record post-analytics processor
                script. This can be a local path or an S3 uri.
            output_s3_uri (str): S3 destination of the constraint_violations and analysis result.
                Default: "s3://<default_session_bucket>/<job_name>/output"
            constraints (sagemaker.model_monitor.Constraints or str): If provided it will be used
                for monitoring the endpoint. It can be a Constraints object or an S3 uri pointing
                to a constraints JSON file.
            schedule_cron_expression (str): The cron expression that dictates the frequency that
                this job run. See sagemaker.model_monitor.CronExpressionGenerator for valid
                expressions. Default: Daily.
            enable_cloudwatch_metrics (bool): Whether to publish cloudwatch metrics as part of
                the baselining or monitoring jobs.
            role (str): An AWS IAM role. The Amazon SageMaker jobs use this role.
            instance_count (int): The number of instances to run
                the jobs with.
            instance_type (str): Type of EC2 instance to use for
                the job, for example, 'ml.m5.xlarge'.
            volume_size_in_gb (int): Size in GB of the EBS volume
                to use for storing data during processing (default: 30).
            volume_kms_key (str): A KMS key for the job's volume.
            output_kms_key (str): The KMS key id for the job's outputs.
            max_runtime_in_seconds (int): Timeout in seconds. After this amount of
                time, Amazon SageMaker terminates the job regardless of its current status.
                Default: 3600
            env (dict): Environment variables to be passed to the job.
            network_config (sagemaker.network.NetworkConfig): A NetworkConfig
                object that configures network isolation, encryption of
                inter-container traffic, security group IDs, and subnets.
            batch_transform_input (sagemaker.model_monitor.BatchTransformInput): Inputs to
                run the monitoring schedule on the batch transform
            data_analysis_start_time (str): Start time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)
            data_analysis_end_time (str): End time for the data analysis window
                for the one time monitoring schedule (NOW), e.g. "-PT1H" (default: None)
        """
        ...
    
    def delete_monitoring_schedule(self):
        """Deletes the monitoring schedule and its job definition."""
        ...
    
    @classmethod
    def attach(cls, monitor_schedule_name, sagemaker_session=...):
        """Sets this object's schedule name to the name provided.

        This allows subsequent describe_schedule or list_executions calls to point
        to the given schedule.

        Args:
            monitor_schedule_name (str): The name of the schedule to attach to.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one is created using
                the default AWS configuration chain.
        """
        ...
    


class BaseliningJob(ProcessingJob):
    """Provides functionality to retrieve baseline-specific files output from baselining job."""
    def __init__(self, sagemaker_session, job_name, inputs, outputs, output_kms_key=...) -> None:
        """Initializes a Baselining job.

        It tracks a baselining job kicked off by the suggest workflow.

        Args:
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one is created using
                the default AWS configuration chain.
            job_name (str): Name of the Amazon SageMaker Model Monitoring Baselining Job.
            inputs ([sagemaker.processing.ProcessingInput]): A list of ProcessingInput objects.
            outputs ([sagemaker.processing.ProcessingOutput]): A list of ProcessingOutput objects.
            output_kms_key (str): The output kms key associated with the job. Defaults to None
                if not provided.

        """
        ...
    
    @classmethod
    def from_processing_job(cls, processing_job):
        """Initializes a Baselining job from a processing job.

        Args:
            processing_job (sagemaker.processing.ProcessingJob): The ProcessingJob used for
                baselining instance.

        Returns:
            sagemaker.processing.BaseliningJob: The instance of ProcessingJob created
                using the current job name.

        """
        ...
    
    def baseline_statistics(self, file_name=..., kms_key=...):
        """Returns a sagemaker.model_monitor.

        Statistics object representing the statistics JSON file generated by this baselining job.

        Args:
            file_name (str): The name of the json-formatted statistics file
            kms_key (str): The kms key to use when retrieving the file.

        Returns:
            sagemaker.model_monitor.Statistics: The Statistics object representing the file that
                was generated by the job.

        Raises:
            UnexpectedStatusException: This is thrown if the job is not in a 'Complete' state.

        """
        ...
    
    def suggested_constraints(self, file_name=..., kms_key=...):
        """Returns a sagemaker.model_monitor.

        Constraints object representing the constraints JSON file generated by this baselining job.

        Args:
            file_name (str): The name of the json-formatted constraints file
            kms_key (str): The kms key to use when retrieving the file.

        Returns:
            sagemaker.model_monitor.Constraints: The Constraints object representing the file that
                was generated by the job.

        Raises:
            UnexpectedStatusException: This is thrown if the job is not in a 'Complete' state.

        """
        ...
    


class MonitoringExecution(ProcessingJob):
    """Provides functionality to retrieve monitoring-specific files from monitoring executions."""
    def __init__(self, sagemaker_session, job_name, inputs, output, output_kms_key=...) -> None:
        """Initializes a MonitoringExecution job that tracks a monitoring execution.

        Its kicked off by an Amazon SageMaker Model Monitoring Schedule.

        Args:
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one is created using
                the default AWS configuration chain.
            job_name (str): The name of the monitoring execution job.
            output (sagemaker.Processing.ProcessingOutput): The output associated with the
                monitoring execution.
            output_kms_key (str): The output kms key associated with the job. Defaults to None
                if not provided.

        """
        ...
    
    @classmethod
    def from_processing_arn(cls, sagemaker_session, processing_job_arn):
        """Initializes a Baselining job from a processing arn.

        Args:
            processing_job_arn (str): ARN of the processing job to create a MonitoringExecution
            out of.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, one is created using
                the default AWS configuration chain.

        Returns:
            sagemaker.processing.BaseliningJob: The instance of ProcessingJob created
                using the current job name.

        """
        ...
    
    def statistics(self, file_name=..., kms_key=...):
        """Returns a sagemaker.model_monitor.

        Statistics object representing the statistics
        JSON file generated by this monitoring execution.

        Args:
            file_name (str): The name of the json-formatted statistics file
            kms_key (str): The kms key to use when retrieving the file.

        Returns:
            sagemaker.model_monitor.Statistics: The Statistics object representing the file that
                was generated by the execution.

        Raises:
            UnexpectedStatusException: This is thrown if the job is not in a 'Complete' state.

        """
        ...
    
    def constraint_violations(self, file_name=..., kms_key=...):
        """Returns a sagemaker.model_monitor.

        ConstraintViolations object representing the constraint violations
        JSON file generated by this monitoring execution.

        Args:
            file_name (str): The name of the json-formatted constraint violations file.
            kms_key (str): The kms key to use when retrieving the file.

        Returns:
            sagemaker.model_monitor.ConstraintViolations: The ConstraintViolations object
                representing the file that was generated by the monitoring execution.

        Raises:
            UnexpectedStatusException: This is thrown if the job is not in a 'Complete' state.

        """
        ...
    


class EndpointInput:
    """Accepts parameters that specify an endpoint input for monitoring execution.

    It also provides a method to turn those parameters into a dictionary.
    """
    def __init__(self, endpoint_name, destination, s3_input_mode=..., s3_data_distribution_type=..., start_time_offset=..., end_time_offset=..., features_attribute=..., inference_attribute=..., probability_attribute=..., probability_threshold_attribute=..., exclude_features_attribute=...) -> None:
        """Initialize an ``EndpointInput`` instance.

        EndpointInput accepts parameters that specify an endpoint input for a monitoring
        job and provides a method to turn those parameters into a dictionary.

        Args:
            endpoint_name (str): The name of the endpoint.
            destination (str): The destination of the input.
            s3_input_mode (str): The S3 input mode. Can be one of: "File", "Pipe. Default: "File".
            s3_data_distribution_type (str): The S3 Data Distribution Type. Can be one of:
                "FullyReplicated", "ShardedByS3Key"
            start_time_offset (str): Monitoring start time offset, e.g. "-PT1H"
            end_time_offset (str): Monitoring end time offset, e.g. "-PT0H".
            features_attribute (str): JSONpath to locate features in JSONlines dataset.
                Only used for ModelBiasMonitor and ModelExplainabilityMonitor
            inference_attribute (str): Index or JSONpath to locate predicted label(s).
                Only used for ModelQualityMonitor, ModelBiasMonitor, and ModelExplainabilityMonitor
            probability_attribute (str or int): Index or JSONpath to locate probabilities.
                Only used for ModelQualityMonitor, ModelBiasMonitor and ModelExplainabilityMonitor
            probability_threshold_attribute (float): threshold to convert probabilities to binaries
                Only used for ModelQualityMonitor, ModelBiasMonitor and ModelExplainabilityMonitor
            exclude_features_attribute (str): Comma separated column indices of features or
                actual feature names that needs to be excluded. (default: None)
        """
        ...
    


@attr.s
class MonitoringInput:
    """Accepts parameters specifying batch transform or endpoint inputs for monitoring execution.

    MonitoringInput accepts parameters that specify additional parameters while monitoring jobs.
    It also provides a method to turn those parameters into a dictionary.

    Args:
        start_time_offset (str): Monitoring start time offset, e.g. "-PT1H"
        end_time_offset (str): Monitoring end time offset, e.g. "-PT0H".
        features_attribute (str): JSONpath to locate features in JSONlines dataset.
            Only used for ModelBiasMonitor and ModelExplainabilityMonitor
        inference_attribute (str): Index or JSONpath to locate predicted label(s).
            Only used for ModelQualityMonitor, ModelBiasMonitor, and ModelExplainabilityMonitor
        probability_attribute (str): Index or JSONpath to locate probabilities.
            Only used for ModelQualityMonitor, ModelBiasMonitor and ModelExplainabilityMonitor
        probability_threshold_attribute (float): threshold to convert probabilities to binaries
            Only used for ModelQualityMonitor, ModelBiasMonitor and ModelExplainabilityMonitor
    """
    start_time_offset: str = ...
    end_time_offset: str = ...
    features_attribute: str = ...
    inference_attribute: str = ...
    probability_attribute: Union[str, int] = ...
    probability_threshold_attribute: float = ...


class BatchTransformInput(MonitoringInput):
    """Accepts parameters that specify a batch transform input for monitoring schedule.

    It also provides a method to turn those parameters into a dictionary.
    """
    def __init__(self, data_captured_destination_s3_uri: str, destination: str, dataset_format: MonitoringDatasetFormat, s3_input_mode: str = ..., s3_data_distribution_type: str = ..., start_time_offset: str = ..., end_time_offset: str = ..., features_attribute: str = ..., inference_attribute: str = ..., probability_attribute: str = ..., probability_threshold_attribute: str = ..., exclude_features_attribute: str = ...) -> None:
        """Initialize a `BatchTransformInput` instance.

        Args:
            data_captured_destination_s3_uri (str): Location to the batch transform captured data
                file which needs to be analysed.
            destination (str): The destination of the input.
            s3_input_mode (str): The S3 input mode. Can be one of: "File", "Pipe. (default: File)
            s3_data_distribution_type (str): The S3 Data Distribution Type. Can be one of:
                "FullyReplicated", "ShardedByS3Key" (default: FullyReplicated)
            start_time_offset (str): Monitoring start time offset, e.g. "-PT1H" (default: None)
            end_time_offset (str): Monitoring end time offset, e.g. "-PT0H". (default: None)
            features_attribute (str): JSONpath to locate features in JSONlines dataset.
                Only used for ModelBiasMonitor and ModelExplainabilityMonitor (default: None)
            inference_attribute (str): Index or JSONpath to locate predicted label(s).
                Only used for ModelQualityMonitor, ModelBiasMonitor, and ModelExplainabilityMonitor
                (default: None)
            probability_attribute (str): Index or JSONpath to locate probabilities.
                Only used for ModelQualityMonitor, ModelBiasMonitor and ModelExplainabilityMonitor
                (default: None)
            probability_threshold_attribute (float): threshold to convert probabilities to binaries
                Only used for ModelQualityMonitor, ModelBiasMonitor and ModelExplainabilityMonitor
                (default: None)
            exclude_features_attribute (str): Comma separated column indices of features or
                actual feature names that needs to be excluded. (default: None)

        """
        ...
    


class MonitoringOutput:
    """Accepts parameters that specify an S3 output for a monitoring job.

    It also provides a method to turn those parameters into a dictionary.
    """
    def __init__(self, source, destination=..., s3_upload_mode=...) -> None:
        """Initialize a ``MonitoringOutput`` instance.

        MonitoringOutput accepts parameters that specify an S3 output for a monitoring
        job and provides a method to turn those parameters into a dictionary.

        Args:
            source (str): The source for the output.
            destination (str): The destination of the output. Optional.
                Default: s3://<default-session-bucket/schedule_name/output
            s3_upload_mode (str): The S3 upload mode.

        """
        ...
    


