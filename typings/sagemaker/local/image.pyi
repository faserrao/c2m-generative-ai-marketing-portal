"""
This type stub file was generated by pyright.
"""

from threading import Thread

"""Placeholder docstring"""
CONTAINER_PREFIX = ...
DOCKER_COMPOSE_FILENAME = ...
DOCKER_COMPOSE_HTTP_TIMEOUT_ENV = ...
DOCKER_COMPOSE_HTTP_TIMEOUT = ...
REGION_ENV_NAME = ...
TRAINING_JOB_NAME_ENV_NAME = ...
S3_ENDPOINT_URL_ENV_NAME = ...
SELINUX_ENABLED = ...
logger = ...
class _SageMakerContainer:
    """Handle the lifecycle and configuration of a local container execution.

    This class is responsible for creating the directories and configuration
    files that the docker containers will use for either training or serving.
    """
    def __init__(self, instance_type, instance_count, image, sagemaker_session=..., container_entrypoint=..., container_arguments=...) -> None:
        """Initialize a SageMakerContainer instance

        It uses a :class:`sagemaker.session.Session` for general interaction
        with user configuration such as getting the default sagemaker S3 bucket.
        However this class does not call any of the SageMaker APIs.

        Args:
            instance_type (str): The instance type to use. Either 'local' or
                'local_gpu'
            instance_count (int): The number of instances to create.
            image (str): docker image to use.
            sagemaker_session (sagemaker.session.Session): a sagemaker session
                to use when interacting with SageMaker.
            container_entrypoint (str): the container entrypoint to execute
            container_arguments (str): the container entrypoint arguments
        """
        ...
    
    def process(self, processing_inputs, processing_output_config, environment, processing_job_name):
        """Run a processing job locally using docker-compose.

        Args:
            processing_inputs (dict): The processing input specification.
            processing_output_config (dict): The processing output configuration specification.
            environment (dict): The environment collection for the processing job.
            processing_job_name (str): Name of the local processing job being run.
        """
        ...
    
    def train(self, input_data_config, output_data_config, hyperparameters, environment, job_name):
        """Run a training job locally using docker-compose.

        Args:
            input_data_config (dict): The Input Data Configuration, this contains data such as the
                channels to be used for training.
            output_data_config: The configuration of the output data.
            hyperparameters (dict): The HyperParameters for the training job.
            environment (dict): The environment collection for the training job.
            job_name (str): Name of the local training job being run.

        Returns (str): Location of the trained model.
        """
        ...
    
    def serve(self, model_dir, environment):
        """Host a local endpoint using docker-compose.

        Args:
            primary_container (dict): dictionary containing the container runtime settings
                for serving. Expected keys:
                - 'ModelDataUrl' pointing to a file or s3:// location.
                - 'Environment' a dictionary of environment variables to be passed to the
                    hosting container.
        """
        ...
    
    def stop_serving(self):
        """Stop the serving container.

        The serving container runs in async mode to allow the SDK to do other
        tasks.
        """
        ...
    
    def retrieve_artifacts(self, compose_data, output_data_config, job_name):
        """Get the model artifacts from all the container nodes.

        Used after training completes to gather the data from all the
        individual containers. As the official SageMaker Training Service, it
        will override duplicate files if multiple containers have the same file
        names.

        Args:
            compose_data (dict): Docker-Compose configuration in dictionary
                format.
            output_data_config: The configuration of the output data.
            job_name: The name of the job.

        Returns: Local path to the collected model artifacts.
        """
        ...
    
    def write_processing_config_files(self, host, environment, processing_inputs, processing_output_config, processing_job_name):
        """Write the config files for the processing containers.

        This method writes the hyperparameters, resources and input data
        configuration files.

        Args:
            host (str): Host to write the configuration for
            environment (dict): Environment variable collection.
            processing_inputs (dict): Processing inputs.
            processing_output_config (dict): Processing output configuration.
            processing_job_name (str): Processing job name.
        """
        ...
    
    def write_config_files(self, host, hyperparameters, input_data_config):
        """Write the config files for the training containers.

        This method writes the hyperparameters, resources and input data
        configuration files.

        Returns: None

        Args:
            host (str): Host to write the configuration for
            hyperparameters (dict): Hyperparameters for training.
            input_data_config (dict): Training input channels to be used for
                training.
        """
        ...
    


class _HostingContainer(Thread):
    """Placeholder docstring."""
    def __init__(self, command) -> None:
        """Creates a new threaded hosting container.

        Args:
            command:
        """
        ...
    
    def run(self):
        """Placeholder docstring"""
        ...
    
    def down(self):
        """Placeholder docstring"""
        ...
    


class _Volume:
    """Represent a Volume that will be mapped to a container."""
    def __init__(self, host_dir, container_dir=..., channel=...) -> None:
        """Create a Volume instance.

        The container path can be provided as a container_dir or as a channel name but not both.

        Args:
            host_dir (str): path to the volume data in the host
            container_dir (str): path inside the container that host_dir will be mapped to
            channel (str): channel name that the host_dir represents. It will be mapped as
                /opt/ml/input/data/<channel> in the container.
        """
        ...
    


