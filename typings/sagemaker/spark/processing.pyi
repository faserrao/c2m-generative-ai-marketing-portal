"""
This type stub file was generated by pyright.
"""

from enum import Enum
from typing import Dict, List, Optional, Union
from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor
from sagemaker.session import Session
from sagemaker.network import NetworkConfig
from sagemaker.workflow.pipeline_context import runnable_by_pipeline
from sagemaker.workflow.entities import PipelineVariable

"""
This type stub file was generated by pyright.
"""
logger = ...
class _SparkProcessorBase(ScriptProcessor):
    """Handles Amazon SageMaker processing tasks for jobs using Spark.

    Base class for either PySpark or SparkJars.
    """
    _default_command = ...
    _conf_container_base_path = ...
    _conf_container_input_name = ...
    _conf_file_name = ...
    _submit_jars_input_channel_name = ...
    _submit_files_input_channel_name = ...
    _submit_py_files_input_channel_name = ...
    _submit_deps_error_message = ...
    _history_server_port = ...
    _history_server_url_suffix = ...
    _spark_event_log_default_local_path = ...
    def __init__(self, role=..., instance_type=..., instance_count=..., framework_version=..., py_version=..., container_version=..., image_uri=..., volume_size_in_gb=..., volume_kms_key=..., output_kms_key=..., configuration_location: Optional[str] = ..., dependency_location: Optional[str] = ..., max_runtime_in_seconds=..., base_job_name=..., sagemaker_session=..., env=..., tags=..., network_config=...) -> None:
        """Initialize a ``_SparkProcessorBase`` instance.

        The _SparkProcessorBase handles Amazon SageMaker processing tasks for
        jobs using SageMaker Spark.

        Args:
            framework_version (str): The version of SageMaker PySpark.
            py_version (str): The version of python.
            container_version (str): The version of spark container.
            role (str): An AWS IAM role name or ARN. Amazon SageMaker Processing
                uses this role to access AWS resources, such as
                data stored in Amazon S3 (default: None).
                If not specified, the value from the defaults configuration file
                will be used.
            instance_type (str): Type of EC2 instance to use for
                processing, for example, 'ml.c4.xlarge'.
            instance_count (int): The number of instances to run
                the Processing job with. Defaults to 1.
            volume_size_in_gb (int): Size in GB of the EBS volume to
                use for storing data during processing (default: 30).
            volume_kms_key (str): A KMS key for the processing
                volume.
            output_kms_key (str): The KMS key id for all ProcessingOutputs.
            configuration_location (str): The S3 prefix URI where the user-provided EMR
                application configuration will be uploaded (default: None). If not specified,
                the default ``configuration location`` is 's3://{sagemaker-default-bucket}'.
            dependency_location (str): The S3 prefix URI where Spark dependencies will be
                uploaded (default: None). If not specified, the default ``dependency location``
                is 's3://{sagemaker-default-bucket}'.
            max_runtime_in_seconds (int): Timeout in seconds.
                After this amount of time Amazon SageMaker terminates the job
                regardless of its current status.
            base_job_name (str): Prefix for processing name. If not specified,
                the processor generates a default job name, based on the
                training image name and current timestamp.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon
                SageMaker APIs and any other AWS services needed. If not specified,
                the processor creates one using the default AWS configuration chain.
            env (dict): Environment variables to be passed to the processing job.
            tags ([dict]): List of tags to be passed to the processing job.
            network_config (sagemaker.network.NetworkConfig): A NetworkConfig
                object that configures network isolation, encryption of
                inter-container traffic, security group IDs, and subnets.
        """
        ...
    
    def get_run_args(self, code, inputs=..., outputs=..., arguments=...):
        """Returns a RunArgs object.

        For processors (:class:`~sagemaker.spark.processing.PySparkProcessor`,
            :class:`~sagemaker.spark.processing.SparkJar`) that have special
            run() arguments, this object contains the normalized arguments for passing to
            :class:`~sagemaker.workflow.steps.ProcessingStep`.

        Args:
            code (str): This can be an S3 URI or a local path to a file with the framework
                script to run.
            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
                the processing job. These must be provided as
                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).
            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
                the processing job. These can be specified as either path strings or
                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).
            arguments (list[str]): A list of string arguments to be passed to a
                processing job (default: None).
        """
        ...
    
    @runnable_by_pipeline
    def run(self, submit_app, inputs=..., outputs=..., arguments=..., wait=..., logs=..., job_name=..., experiment_config=..., kms_key=...):
        """Runs a processing job.

        Args:
            submit_app (str): .py or .jar file to submit to Spark as the primary application
            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
                the processing job. These must be provided as
                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).
            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
                the processing job. These can be specified as either path strings or
                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).
            arguments (list[str]): A list of string arguments to be passed to a
                processing job (default: None).
            wait (bool): Whether the call should wait until the job completes (default: True).
            logs (bool): Whether to show the logs produced by the job.
                Only meaningful when wait is True (default: True).
            job_name (str): Processing job name. If not specified, the processor generates
                a default job name, based on the base job name and current timestamp.
            experiment_config (dict[str, str]): Experiment management configuration.
                Optionally, the dict can contain three keys:
                'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.
                The behavior of setting these keys is as follows:
                * If `ExperimentName` is supplied but `TrialName` is not a Trial will be
                automatically created and the job's Trial Component associated with the Trial.
                * If `TrialName` is supplied and the Trial already exists the job's Trial Component
                will be associated with the Trial.
                * If both `ExperimentName` and `TrialName` are not supplied the trial component
                will be unassociated.
                * `TrialComponentDisplayName` is used for display in Studio.
            kms_key (str): The ARN of the KMS key that is used to encrypt the
                user code file (default: None).
        """
        ...
    
    def start_history_server(self, spark_event_logs_s3_uri=...):
        """Starts a Spark history server.

        Args:
            spark_event_logs_s3_uri (str): S3 URI where Spark events are stored.
        """
        ...
    
    def terminate_history_server(self):
        """Terminates the Spark history server."""
        ...
    


class PySparkProcessor(_SparkProcessorBase):
    """Handles Amazon SageMaker processing tasks for jobs using PySpark."""
    def __init__(self, role: str = ..., instance_type: Union[str, PipelineVariable] = ..., instance_count: Union[int, PipelineVariable] = ..., framework_version: Optional[str] = ..., py_version: Optional[str] = ..., container_version: Optional[str] = ..., image_uri: Optional[Union[str, PipelineVariable]] = ..., volume_size_in_gb: Union[int, PipelineVariable] = ..., volume_kms_key: Optional[Union[str, PipelineVariable]] = ..., output_kms_key: Optional[Union[str, PipelineVariable]] = ..., configuration_location: Optional[str] = ..., dependency_location: Optional[str] = ..., max_runtime_in_seconds: Optional[Union[int, PipelineVariable]] = ..., base_job_name: Optional[str] = ..., sagemaker_session: Optional[Session] = ..., env: Optional[Dict[str, Union[str, PipelineVariable]]] = ..., tags: Optional[List[Dict[str, Union[str, PipelineVariable]]]] = ..., network_config: Optional[NetworkConfig] = ...) -> None:
        """Initialize an ``PySparkProcessor`` instance.

        The PySparkProcessor handles Amazon SageMaker processing tasks for jobs
        using SageMaker PySpark.

        Args:
            framework_version (str): The version of SageMaker PySpark.
            py_version (str): The version of python.
            container_version (str): The version of spark container.
            role (str): An AWS IAM role name or ARN. Amazon SageMaker Processing
                uses this role to access AWS resources, such as
                data stored in Amazon S3 (default: None).
                If not specified, the value from the defaults configuration file
                will be used.
            instance_type (str or PipelineVariable): Type of EC2 instance to use for
                processing, for example, 'ml.c4.xlarge'.
            instance_count (int or PipelineVariable): The number of instances to run
                the Processing job with. Defaults to 1.
            volume_size_in_gb (int or PipelineVariable): Size in GB of the EBS volume to
                use for storing data during processing (default: 30).
            volume_kms_key (str or PipelineVariable): A KMS key for the processing
                volume.
            output_kms_key (str or PipelineVariable): The KMS key id for all ProcessingOutputs.
            configuration_location (str): The S3 prefix URI where the user-provided EMR
                application configuration will be uploaded (default: None). If not specified,
                the default ``configuration location`` is 's3://{sagemaker-default-bucket}'.
            dependency_location (str): The S3 prefix URI where Spark dependencies will be
                uploaded (default: None). If not specified, the default ``dependency location``
                is 's3://{sagemaker-default-bucket}'.
            max_runtime_in_seconds (int or PipelineVariable): Timeout in seconds.
                After this amount of time Amazon SageMaker terminates the job
                regardless of its current status.
            base_job_name (str): Prefix for processing name. If not specified,
                the processor generates a default job name, based on the
                training image name and current timestamp.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, the processor creates one
                using the default AWS configuration chain.
            env (dict[str, str] or dict[str, PipelineVariable]): Environment variables to
                be passed to the processing job.
            tags (list[dict[str, str] or list[dict[str, PipelineVariable]]): List of tags to
                be passed to the processing job.
            network_config (sagemaker.network.NetworkConfig): A NetworkConfig
                object that configures network isolation, encryption of
                inter-container traffic, security group IDs, and subnets.
        """
        ...
    
    def get_run_args(self, submit_app, submit_py_files=..., submit_jars=..., submit_files=..., inputs=..., outputs=..., arguments=..., job_name=..., configuration=..., spark_event_logs_s3_uri=...):
        """Returns a RunArgs object.

        This object contains the normalized inputs, outputs and arguments
        needed when using a ``PySparkProcessor`` in a
        :class:`~sagemaker.workflow.steps.ProcessingStep`.

        Args:
            submit_app (str): Path (local or S3) to Python file to submit to Spark
                as the primary application. This is translated to the `code`
                property on the returned `RunArgs` object.
            submit_py_files (list[str]): List of paths (local or S3) to provide for
                `spark-submit --py-files` option
            submit_jars (list[str]): List of paths (local or S3) to provide for
                `spark-submit --jars` option
            submit_files (list[str]): List of paths (local or S3) to provide for
                `spark-submit --files` option
            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
                the processing job. These must be provided as
                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).
            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
                the processing job. These can be specified as either path strings or
                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).
            arguments (list[str]): A list of string arguments to be passed to a
                processing job (default: None).
            job_name (str): Processing job name. If not specified, the processor generates
                a default job name, based on the base job name and current timestamp.
            configuration (list[dict] or dict): Configuration for Hadoop, Spark, or Hive.
                List or dictionary of EMR-style classifications.
                https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html
            spark_event_logs_s3_uri (str): S3 path where spark application events will
                be published to.
        """
        ...
    
    @runnable_by_pipeline
    def run(self, submit_app: str, submit_py_files: Optional[List[Union[str, PipelineVariable]]] = ..., submit_jars: Optional[List[Union[str, PipelineVariable]]] = ..., submit_files: Optional[List[Union[str, PipelineVariable]]] = ..., inputs: Optional[List[ProcessingInput]] = ..., outputs: Optional[List[ProcessingOutput]] = ..., arguments: Optional[List[Union[str, PipelineVariable]]] = ..., wait: bool = ..., logs: bool = ..., job_name: Optional[str] = ..., experiment_config: Optional[Dict[str, str]] = ..., configuration: Optional[Union[List[Dict], Dict]] = ..., spark_event_logs_s3_uri: Optional[Union[str, PipelineVariable]] = ..., kms_key: Optional[str] = ...):
        """Runs a processing job.

        Args:
            submit_app (str): Path (local or S3) to Python file to submit to Spark
                as the primary application
            submit_py_files (list[str] or list[PipelineVariable]): List of paths (local or S3)
                to provide for `spark-submit --py-files` option
            submit_jars (list[str] or list[PipelineVariable]): List of paths (local or S3)
                to provide for `spark-submit --jars` option
            submit_files (list[str] or list[PipelineVariable]): List of paths (local or S3)
                to provide for `spark-submit --files` option
            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
                the processing job. These must be provided as
                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).
            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
                the processing job. These can be specified as either path strings or
                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).
            arguments (list[str] or list[PipelineVariable]): A list of string arguments to
                be passed to a processing job (default: None).
            wait (bool): Whether the call should wait until the job completes (default: True).
            logs (bool): Whether to show the logs produced by the job.
                Only meaningful when wait is True (default: True).
            job_name (str): Processing job name. If not specified, the processor generates
                a default job name, based on the base job name and current timestamp.
            experiment_config (dict[str, str]): Experiment management configuration.
                Optionally, the dict can contain three keys:
                'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.
                The behavior of setting these keys is as follows:
                * If `ExperimentName` is supplied but `TrialName` is not a Trial will be
                automatically created and the job's Trial Component associated with the Trial.
                * If `TrialName` is supplied and the Trial already exists the job's Trial Component
                will be associated with the Trial.
                * If both `ExperimentName` and `TrialName` are not supplied the trial component
                will be unassociated.
                * `TrialComponentDisplayName` is used for display in Studio.
            configuration (list[dict] or dict): Configuration for Hadoop, Spark, or Hive.
                List or dictionary of EMR-style classifications.
                https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html
            spark_event_logs_s3_uri (str or PipelineVariable): S3 path where spark application
                events will be published to.
            kms_key (str): The ARN of the KMS key that is used to encrypt the
                user code file (default: None).
        """
        ...
    


class SparkJarProcessor(_SparkProcessorBase):
    """Handles Amazon SageMaker processing tasks for jobs using Spark with Java or Scala Jars."""
    def __init__(self, role: str = ..., instance_type: Union[str, PipelineVariable] = ..., instance_count: Union[int, PipelineVariable] = ..., framework_version: Optional[str] = ..., py_version: Optional[str] = ..., container_version: Optional[str] = ..., image_uri: Optional[Union[str, PipelineVariable]] = ..., volume_size_in_gb: Union[int, PipelineVariable] = ..., volume_kms_key: Optional[Union[str, PipelineVariable]] = ..., output_kms_key: Optional[Union[str, PipelineVariable]] = ..., configuration_location: Optional[str] = ..., dependency_location: Optional[str] = ..., max_runtime_in_seconds: Optional[Union[int, PipelineVariable]] = ..., base_job_name: Optional[str] = ..., sagemaker_session: Optional[Session] = ..., env: Optional[Dict[str, Union[str, PipelineVariable]]] = ..., tags: Optional[List[Dict[str, Union[str, PipelineVariable]]]] = ..., network_config: Optional[NetworkConfig] = ...) -> None:
        """Initialize a ``SparkJarProcessor`` instance.

        The SparkProcessor handles Amazon SageMaker processing tasks for jobs
        using SageMaker Spark.

        Args:
            framework_version (str): The version of SageMaker PySpark.
            py_version (str): The version of python.
            container_version (str): The version of spark container.
            role (str): An AWS IAM role name or ARN. Amazon SageMaker Processing
                uses this role to access AWS resources, such as
                data stored in Amazon S3 (default: None).
                If not specified, the value from the defaults configuration file
                will be used.
            instance_type (str or PipelineVariable): Type of EC2 instance to use for
                processing, for example, 'ml.c4.xlarge'.
            instance_count (int or PipelineVariable): The number of instances to run
                the Processing job with. Defaults to 1.
            volume_size_in_gb (int or PipelineVariable): Size in GB of the EBS volume to
                use for storing data during processing (default: 30).
            volume_kms_key (str or PipelineVariable): A KMS key for the processing
                volume.
            output_kms_key (str or PipelineVariable): The KMS key id for all ProcessingOutputs.
            configuration_location (str): The S3 prefix URI where the user-provided EMR
                application configuration will be uploaded (default: None). If not specified,
                the default ``configuration location`` is 's3://{sagemaker-default-bucket}'.
            dependency_location (str): The S3 prefix URI where Spark dependencies will be
                uploaded (default: None). If not specified, the default ``dependency location``
                is 's3://{sagemaker-default-bucket}'.
            max_runtime_in_seconds (int or PipelineVariable): Timeout in seconds.
                After this amount of time Amazon SageMaker terminates the job
                regardless of its current status.
            base_job_name (str): Prefix for processing name. If not specified,
                the processor generates a default job name, based on the
                training image name and current timestamp.
            sagemaker_session (sagemaker.session.Session): Session object which
                manages interactions with Amazon SageMaker APIs and any other
                AWS services needed. If not specified, the processor creates one
                using the default AWS configuration chain.
            env (dict[str, str] or dict[str, PipelineVariable]): Environment variables to
                be passed to the processing job.
            tags (list[dict[str, str] or list[dict[str, PipelineVariable]]): List of tags to
                be passed to the processing job.
            network_config (sagemaker.network.NetworkConfig): A NetworkConfig
                object that configures network isolation, encryption of
                inter-container traffic, security group IDs, and subnets.
        """
        ...
    
    def get_run_args(self, submit_app, submit_class=..., submit_jars=..., submit_files=..., inputs=..., outputs=..., arguments=..., job_name=..., configuration=..., spark_event_logs_s3_uri=...):
        """Returns a RunArgs object.

        This object contains the normalized inputs, outputs and arguments
        needed when using a ``SparkJarProcessor`` in a
        :class:`~sagemaker.workflow.steps.ProcessingStep`.

        Args:
            submit_app (str): Path (local or S3) to Python file to submit to Spark
                as the primary application. This is translated to the `code`
                property on the returned `RunArgs` object
            submit_class (str): Java class reference to submit to Spark as the primary
                application
            submit_jars (list[str]): List of paths (local or S3) to provide for
                `spark-submit --jars` option
            submit_files (list[str]): List of paths (local or S3) to provide for
                `spark-submit --files` option
            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
                the processing job. These must be provided as
                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).
            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
                the processing job. These can be specified as either path strings or
                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).
            arguments (list[str]): A list of string arguments to be passed to a
                processing job (default: None).
            job_name (str): Processing job name. If not specified, the processor generates
                a default job name, based on the base job name and current timestamp.
            configuration (list[dict] or dict): Configuration for Hadoop, Spark, or Hive.
                List or dictionary of EMR-style classifications.
                https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html
            spark_event_logs_s3_uri (str): S3 path where spark application events will
                be published to.
        """
        ...
    
    @runnable_by_pipeline
    def run(self, submit_app: str, submit_class: Union[str, PipelineVariable], submit_jars: Optional[List[Union[str, PipelineVariable]]] = ..., submit_files: Optional[List[Union[str, PipelineVariable]]] = ..., inputs: Optional[List[ProcessingInput]] = ..., outputs: Optional[List[ProcessingOutput]] = ..., arguments: Optional[List[Union[str, PipelineVariable]]] = ..., wait: bool = ..., logs: bool = ..., job_name: Optional[str] = ..., experiment_config: Optional[Dict[str, str]] = ..., configuration: Optional[Union[List[Dict], Dict]] = ..., spark_event_logs_s3_uri: Optional[Union[str, PipelineVariable]] = ..., kms_key: Optional[str] = ...):
        """Runs a processing job.

        Args:
            submit_app (str): Path (local or S3) to Jar file to submit to Spark as
                the primary application
            submit_class (str or PipelineVariable): Java class reference to submit to Spark
                as the primary application
            submit_jars (list[str] or list[PipelineVariable]): List of paths (local or S3)
                to provide for `spark-submit --jars` option
            submit_files (list[str] or list[PipelineVariable]): List of paths (local or S3)
                to provide for `spark-submit --files` option
            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
                the processing job. These must be provided as
                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).
            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
                the processing job. These can be specified as either path strings or
                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).
            arguments (list[str] or list[PipelineVariable]): A list of string arguments to
                be passed to a processing job (default: None).
            wait (bool): Whether the call should wait until the job completes (default: True).
            logs (bool): Whether to show the logs produced by the job.
                Only meaningful when wait is True (default: True).
            job_name (str): Processing job name. If not specified, the processor generates
                a default job name, based on the base job name and current timestamp.
            experiment_config (dict[str, str]): Experiment management configuration.
                Optionally, the dict can contain three keys:
                'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.
                The behavior of setting these keys is as follows:
                * If `ExperimentName` is supplied but `TrialName` is not a Trial will be
                automatically created and the job's Trial Component associated with the Trial.
                * If `TrialName` is supplied and the Trial already exists the job's Trial Component
                will be associated with the Trial.
                * If both `ExperimentName` and `TrialName` are not supplied the trial component
                will be unassociated.
                * `TrialComponentDisplayName` is used for display in Studio.
            configuration (list[dict] or dict): Configuration for Hadoop, Spark, or Hive.
                List or dictionary of EMR-style classifications.
                https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html
            spark_event_logs_s3_uri (str or PipelineVariable): S3 path where spark application
                events will be published to.
            kms_key (str): The ARN of the KMS key that is used to encrypt the
                user code file (default: None).
        """
        ...
    


class _HistoryServer:
    """History server class that is responsible for starting history server."""
    _container_name = ...
    _entry_point = ...
    arg_event_logs_s3_uri = ...
    arg_remote_domain_name = ...
    _history_server_args_format_map = ...
    def __init__(self, cli_args, image_uri, network_config) -> None:
        ...
    
    def run(self):
        """Runs the history server."""
        ...
    
    def down(self):
        """Stops and removes the container."""
        ...
    


class FileType(Enum):
    """Enum of file type"""
    JAR = ...
    PYTHON = ...
    FILE = ...


class SparkConfigUtils:
    """Util class for spark configurations"""
    _valid_configuration_keys = ...
    _valid_configuration_classifications = ...
    @staticmethod
    def validate_configuration(configuration: Dict):
        """Validates the user-provided Hadoop/Spark/Hive configuration.

        This ensures that the list or dictionary the user provides will serialize to
        JSON matching the schema of EMR's application configuration

        Args:
            configuration (Dict): A dict that contains the configuration overrides to
                the default values. For more information, please visit:
                https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html
        """
        ...
    
    @staticmethod
    def validate_s3_uri(spark_output_s3_path):
        """Validate whether the URI uses an S3 scheme.

        In the future, this validation will perform deeper S3 validation.

        Args:
            spark_output_s3_path (str): The URI of the Spark output S3 Path.
        """
        ...
    


